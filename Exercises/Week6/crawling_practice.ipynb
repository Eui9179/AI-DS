{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c82715",
   "metadata": {},
   "source": [
    "## 필요 라이브러리\n",
    "\n",
    "- urllib\n",
    "- BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5218dc",
   "metadata": {},
   "source": [
    "## urllib\n",
    "\n",
    "- 인터넷에서 데이터를 받아 오는 기능\n",
    "\n",
    "\n",
    "- `urllib`에 인터넷 주소(URL)을 넣고 실행하면 데이터를 텍스트 형태로 받아옴\n",
    "\n",
    "```python\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(url)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140da34e",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "\n",
    "- 받아온 데이터에서 필요한 데이터를 추출하는 데 필요한 기능이 들어있는 라이브러리\n",
    "- 외부 라이브러리\n",
    "\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "(아나콘다에 라이브러리가 설치되어 있기 때문에 아니콘다 환경에서는 설치가 필요없음)\n",
    "```\n",
    "```python\n",
    "from bs4 import Beautifulsoup\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb694d1",
   "metadata": {},
   "source": [
    "### BeautifulSoup() 사용하기\n",
    "\n",
    "- BeautifulSoup(<받은 텍스트>, <텍스트를 파싱할 파서>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e8244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<html><div>hello</div></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = \"<html><div>hello</div></html>\"\n",
    "soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "\n",
    "print(type(soup))\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a2f59",
   "metadata": {},
   "source": [
    "### find()\n",
    "\n",
    "- find()를 이용하여 파싱된 데이터 중 필요한 부분만 뽑아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc309df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div>hello</div>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find(\"div\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c7d591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ul>\n",
      "<li>line1</li>\n",
      "<li>line2</li>\n",
      "<li>line3</li>\n",
      "</ul> \n",
      "\n",
      "<li>line1</li> \n",
      "\n",
      "line1\n"
     ]
    }
   ],
   "source": [
    "html_str = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul>\n",
    "            <li>line1</li>\n",
    "            <li>line2</li>\n",
    "            <li>line3</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "\n",
    "ul = soup.find(\"ul\") # 첫번째거 반환\n",
    "print(ul, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189879a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>line1</li> \n",
      "\n",
      "line1\n"
     ]
    }
   ],
   "source": [
    "li = ul.find(\"li\")\n",
    "print(li, \"\\n\")\n",
    "\n",
    "print(li.text) # 테그 제거하고 안에 있는 텍스트만 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971057a4",
   "metadata": {},
   "source": [
    "### find_all()\n",
    "\n",
    "- 조건에 해당하는 모든 요소를 리스트 형태로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e600fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<li>line1</li>, <li>line2</li>, <li>line3</li>]\n"
     ]
    }
   ],
   "source": [
    "lis = ul.find_all(\"li\")\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39484bde",
   "metadata": {},
   "source": [
    "## class 속성 이용하기\n",
    "\n",
    "- 데이터 뽑을 때 class 속성 이용하기\n",
    "\n",
    "```python\n",
    "foo = soup.find(\"div\", {\"class\":\"class1\"})\n",
    "```\n",
    "\n",
    "- 속성값 추출\n",
    "\n",
    "```python\n",
    "url = soup.find(\"a\")\n",
    "ulr[\"href\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1726843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"class1\">class1</div> \n",
      "\n",
      "네이버\n",
      "https://naver.com\n"
     ]
    }
   ],
   "source": [
    "html_str = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <a href=\"https://naver.com\">네이버</a>\n",
    "        <div class=\"class1\">class1</div>\n",
    "        <div class=\"class2\">class2</div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_str, \"html.parser\")\n",
    "div = soup.find(\"div\", {\"class\":\"class1\"})\n",
    "print(div, \"\\n\")\n",
    "\n",
    "url = soup.find(\"a\")\n",
    "print(url.text)\n",
    "print(url['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee02635",
   "metadata": {},
   "source": [
    "## find()와 select()\n",
    "\n",
    "- find()와 find_all()\n",
    "    - find(): 조건을 만족하는 첫 번째 **태그** 리턴\n",
    "    - find_all(): 조건에 해당하는 모든 요소를 리스트로 리턴\n",
    "\n",
    "\n",
    "- select 함수를 사용하여 원하는 데이터 추출하기\n",
    "    - `css_selector`를 활용해서 원하는 태그를 찾는 방법\n",
    "\n",
    "\n",
    "- BeautifulSoup으로 html 문서를 파싱하는 방법\n",
    "    - find(): html tag를 통한 크롤링\n",
    "    - select(): css를 통해 크롤링"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
