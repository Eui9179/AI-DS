{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1df56f",
   "metadata": {},
   "source": [
    "# 크롤링(Crawling)\n",
    "\n",
    "웹 페이지를 가져와서 데이터를 추출하는 행위\n",
    "\n",
    ">데이터를 얻는 방법\n",
    ">1. 파일다운로드\n",
    ">2. api 이용\n",
    ">3. 웹사이트 크롤링/스크래핑\n",
    "\n",
    "## 웹 기초\n",
    "1. HTTP는 HTML로 작성되어 있는 하이퍼 텍스트를 전송하기 위한 프로토콜\n",
    "2. HTTPS는 SSL(보안 소켓 계층)을 사용하여 연결, 서버와 브라우저 사이에 안전하게 암호화된 연결을 만들 수 있게 도와줌\n",
    "3. URL은 네트워크상에서 자원의 위치를 알려주는 주소\n",
    "4. HTML은 웹 페이지를 위한 마크업 언어\n",
    "\n",
    "## 웹 크롤링과 스크래핑\n",
    "\n",
    "**크롤링**은 검색 엔진이 웹 페이지의 정보를 수집하여 분류 및 색인화하여 DB화하는 작업 \n",
    "<BR> \n",
    "크롤러 혹은 봇이 웹 페이지를 방문하여 데이터 수집 \n",
    "    \n",
    "**스크래핑**은 웹 페이지의 정보를 수집하는 일련의 행위, 스크래핑은 크롤링을 포함하는 용어\n",
    "<br>\n",
    "특정한 웹 페이지에서 원하는 데이터 일부를 가져오는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daabe20",
   "metadata": {},
   "source": [
    "# 크롤링을 위한 필요 라이브러리\n",
    "\n",
    "- urllib\n",
    "- BeautifulSoup\n",
    "\n",
    "## urllib\n",
    "\n",
    "url을 넣고 실행하면 데이터를 텍스트 형태로 받아옴\n",
    "\n",
    "## BeautifulSoup\n",
    "\n",
    "데이터를 추출하는 데 필요한 기능이 들어 있는 라이브러리(파싱 라이브러리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5b3c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n",
      "<html><div>hello</div></html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = '''<html><div>hello</div></html>'''\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "print(type(soup))\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cee66c",
   "metadata": {},
   "source": [
    "## find(), find_all()\n",
    "\n",
    "`find()`를 이용하여 파싱된 데이터 중 필요한 부분만 뽑아내기, 처음 만난 부분 추출\n",
    "\n",
    "`find_all()`조건에 해당하는 모든 요소를 리스트 형태로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6aec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div>hello</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find()\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = '''<html><div>hello</div></html>'''\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "soup.find('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d3ab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1\n",
      "line2\n",
      "line3\n"
     ]
    }
   ],
   "source": [
    "# find_all()\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <ul>\n",
    "            <li>line1</li>\n",
    "            <li>line2</li>\n",
    "            <li>line3</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "li_list = soup.find_all('li')\n",
    "\n",
    "for li in li_list:\n",
    "    print(li.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd247d",
   "metadata": {},
   "source": [
    "## find() - class 속성 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ddfc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"class1\">hello</div>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"class1\">hello</div>\n",
    "        <div class=\"class2\">hello2</div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "print(soup.find('div', {'class': 'class1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4181db3f",
   "metadata": {},
   "source": [
    "## find() - 속성값 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1562ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_str = '''\n",
    "<html>\n",
    "    <body>\n",
    "        <div class=\"class1\" color=\"red\">hello</div>\n",
    "        <div class=\"class2\">hello2</div>\n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "\n",
    "div = soup.find('div', {'class': 'class1'})\n",
    "print(div['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161d0d4",
   "metadata": {},
   "source": [
    "# robots.txt\n",
    "\n",
    "웹 크롤러와 같은 봇(Bot)들의 접근 제어 <br>\n",
    "어떤 페이지의 접근이 허가되고 금지 되는지 텍스트 파일로 기록\n",
    "\n",
    "    User-agent:*\n",
    "    Disallow:/\n",
    "    Allow:/$\n",
    "\n",
    "- robots.txt 위치\n",
    "    - 웹 페이지 루트에 위치\n",
    "    - www.naver.com/robots.txt\n",
    "\n",
    "\n",
    "- User-agent\n",
    "    - 누구/어떤 로봇에 규칙이 적용되는지 지정\n",
    "    - \\* 모든 로봇에 적용\n",
    "\n",
    "\n",
    "- Disallow\n",
    "    - 접근을 금지할 페이지\n",
    "    - /: 전체 웹 페이지\n",
    "\n",
    "\n",
    "- Allow\n",
    "    - 접근을 허용할 페이지\n",
    "    - /$: 루트(메인, 첫)페이지\n",
    "\n",
    "\n",
    "- 크롤링은 금지하지만, 메인 페이지는 예외적으로 허용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2f0e3",
   "metadata": {},
   "source": [
    "모든 로봇에게 접근 허락\n",
    "\n",
    "    User-agent:*\n",
    "    Allow:/\n",
    "\n",
    "모든 로봇에게 접근 금지\n",
    "\n",
    "    User-agent:*\n",
    "    Disallow:/\n",
    "\n",
    "\n",
    "모든 로봇에게 특정 디렉토리 접근 금지\n",
    "\n",
    "    User-agent:*\n",
    "    Disallow:/directory_name1/\n",
    "    \n",
    "    \n",
    "모든 로봇에게 특정 파일 접근 금지\n",
    "\n",
    "    User-agent:*\n",
    "    Disallow:/directory_name1/file_name.html    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
